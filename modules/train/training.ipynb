{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 1.18.3 not found\r\n",
      "Collecting transformers==4.11.3\r\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m48.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (2023.8.8)\r\n",
      "Requirement already satisfied: requests in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (2.31.0)\r\n",
      "Collecting sacremoses (from transformers==4.11.3)\r\n",
      "  Obtaining dependency information for sacremoses from https://files.pythonhosted.org/packages/0b/f0/89ee2bc9da434bd78464f288fdb346bc2932f2ee80a90b2a4bbbac262c74/sacremoses-0.1.1-py3-none-any.whl.metadata\r\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\r\n",
      "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.7/212.7 kB\u001B[0m \u001B[31m66.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from transformers==4.11.3) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from requests->transformers==4.11.3) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from requests->transformers==4.11.3) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from requests->transformers==4.11.3) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from requests->transformers==4.11.3) (2023.7.22)\r\n",
      "Requirement already satisfied: click in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /Users/jama/miniforge3/envs/bart_env/lib/python3.11/site-packages (from sacremoses->transformers==4.11.3) (1.3.2)\r\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m897.5/897.5 kB\u001B[0m \u001B[31m58.2 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: tokenizers\r\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001B[?25l|"
     ]
    }
   ],
   "source": [
    "!pip install datasets>=1.18.3\n",
    "!pip install transformers==4.11.3\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-28T05:19:41.609039Z"
    }
   },
   "id": "18fa1858ce3e29ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cdcc777300c3983c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2ForCTC,\n",
    "    HubertForCTC,\n",
    "    Wav2Vec2ConformerForCTC,\n",
    ")\n",
    "from transformers.trainer import Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "efc863081da71258"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(\n",
    "            self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [\n",
    "            {\"input_values\": feature[\"input_values\"]} for feature in features\n",
    "        ]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e89cc024b789016a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29be818350e9feb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    processor = init_wav2vec_processor(TOKENIZER_PATH)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    table = zip(\n",
    "        pred_str,\n",
    "        label_str,\n",
    "    )\n",
    "    for row in table:\n",
    "        print(row[0], \"\\n\", row[1], \"\\n---------------------------\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19a7b8525bca6b15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_wav2vec(\n",
    "        train_dataset, train_dataset_length, test_dataset, test_dataset_length\n",
    "):\n",
    "    processor = init_wav2vec_processor(TOKENIZER_PATH)\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    LEARNING_RATE = float(os.getenv(\"LEARNING_RATE\", 1e-4))\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    batch_size = int(os.getenv(\"BATCH_SIZE\", 4))\n",
    "    gradient_accumulation_steps = int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", 1))\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_PATH,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=2,\n",
    "        eval_steps=max(\n",
    "            int(\n",
    "                train_dataset_length / batch_size / gradient_accumulation_steps / 2 / 30\n",
    "            ),\n",
    "            30,\n",
    "        ),\n",
    "        save_steps=max(\n",
    "            int(\n",
    "                train_dataset_length / batch_size / gradient_accumulation_steps / 2 / 30\n",
    "            ),\n",
    "            30,\n",
    "        ),\n",
    "        max_steps=int(\n",
    "            train_dataset_length\n",
    "            / batch_size\n",
    "            / gradient_accumulation_steps\n",
    "            / 2\n",
    "            * int(os.getenv(\"NUM_TRAIN_EPOCHS\", 1))\n",
    "        ),\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=int(os.getenv(\"NUM_TRAIN_EPOCHS\", 1)),\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=0,\n",
    "        save_total_limit=1,\n",
    "        push_to_hub=False,\n",
    "        report_to=[\n",
    "            \"tensorboard\",\n",
    "        ],\n",
    "        # metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        dataloader_num_workers=0 if os.name == \"nt\" else os.cpu_count() * 2,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        ignore_data_skip=True,\n",
    "        sharded_ddp=[\"simple\"] if os.name != \"nt\" else [],\n",
    "    )\n",
    "    # model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    # model = HubertForCTC.from_pretrained(\n",
    "    model = Wav2Vec2ConformerForCTC.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        attention_dropout=0.0,\n",
    "        hidden_dropout=0.0,\n",
    "        feat_proj_dropout=0.0,\n",
    "        mask_time_prob=0.05,\n",
    "        layerdrop=0.0,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer),\n",
    "        # output_hidden_size=768,\n",
    "    )\n",
    "    model.freeze_feature_encoder()\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    processor.save_pretrained(training_args.output_dir)\n",
    "    last_checkpoint = None\n",
    "    # last_checkpoint = get_last_checkpoint(args.output_dir)\n",
    "    print(\"Starting to train\", last_checkpoint)\n",
    "    trainer.train(\n",
    "        resume_from_checkpoint=last_checkpoint,\n",
    "    )\n",
    "    final_path = os.path.join(training_args.output_dir, \"last_checkpoint\")\n",
    "    trainer.save_model(os.path.join(final_path))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd317d479ae073c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"OUTPUT_PATH\", OUTPUT_PATH)\n",
    "    print(\"BASE_MODEL\", BASE_MODEL)\n",
    "    print(\"TOKENIZER_PATH\", TOKENIZER_PATH)\n",
    "    print(\"CLIPS_PATH\", CLIPS_PATH)\n",
    "    print(\"TRAIN_DATASET_PATH\", TRAIN_DATASET_PATH)\n",
    "    print(\"TEST_DATASET_PATH\", TEST_DATASET_PATH)\n",
    "    print(\"VALIDATE_DATASET_PATH\", VALIDATE_DATASET_PATH)\n",
    "    print(\"CACHE_DIR\", CACHE_DIR)\n",
    "\n",
    "    train_dataset = Dataset.from_json(TRAIN_DATASET_PATH, cache_dir=CACHE_DIR)\n",
    "    # train_dataset = train_dataset.select(range(16688, len(train_dataset)))\n",
    "    train_dataset_length = len(train_dataset)\n",
    "    test_dataset = Dataset.from_json(TEST_DATASET_PATH, cache_dir=CACHE_DIR)\n",
    "    validate_dataset = Dataset.from_json(VALIDATE_DATASET_PATH, cache_dir=CACHE_DIR)\n",
    "    validate_dataset = validate_dataset.select_columns(\n",
    "        [\n",
    "            \"path\",\n",
    "            \"original_sentence\",\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = train_dataset.select_columns(\n",
    "        [\n",
    "            \"path\",\n",
    "            \"original_sentence\",\n",
    "        ]\n",
    "    )\n",
    "    test_dataset = test_dataset.select_columns(\n",
    "        [\n",
    "            \"path\",\n",
    "            \"original_sentence\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = streamify_dataset(train_dataset)\n",
    "\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    # wav2vec_vocab(train_dataset)\n",
    "    init_wav2vec_processor(TOKENIZER_PATH)\n",
    "\n",
    "    validate_dataset = validate_dataset.map(\n",
    "        generate_sentence_data,\n",
    "        num_proc=os.cpu_count(),\n",
    "        disable_nullable=True,\n",
    "        cache_file_name=os.path.join(CACHE_DIR, f\"t-wav2vec2-validate.arrow\"),\n",
    "        load_from_cache_file=False,\n",
    "        writer_batch_size=500,\n",
    "        remove_columns=validate_dataset.column_names,\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        generate_sentence_data,\n",
    "        num_proc=os.cpu_count(),\n",
    "        disable_nullable=True,\n",
    "        cache_file_name=os.path.join(CACHE_DIR, f\"t-wav2vec2-test.arrow\"),\n",
    "        load_from_cache_file=False,\n",
    "        writer_batch_size=500,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "    )\n",
    "    # dictionary = train_dataset.map(\n",
    "    #     extract_all_chars,\n",
    "    #     batched=True,\n",
    "    #     batch_size=-1,\n",
    "    #     keep_in_memory=True,\n",
    "    #     remove_columns=train_dataset.column_names,\n",
    "    # )\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        generate_sentence_data,\n",
    "        # num_proc=os.cpu_count(),\n",
    "        # disable_nullable=True,\n",
    "        # cache_file_name=os.path.join(CACHE_DIR, f\"t-wav2vec2-train.arrow\"),\n",
    "        # load_from_cache_file=False,\n",
    "        # writer_batch_size=500,\n",
    "        # remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(seed=24)\n",
    "\n",
    "    train_wav2vec(\n",
    "        train_dataset,\n",
    "        train_dataset_length,\n",
    "        validate_dataset,\n",
    "        len(validate_dataset),\n",
    "    )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca7dc877237b33e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
